---
layout:     post
title:      《机器学习》之激活函数家族
subtitle:   常用激活函数介绍与对比
date:       2020-07-10
author:     予以初始
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 深度学习
    - 机器学习
---

## 介绍

激活函数是机器学习与深度学习模型必不可少的一部分，用于将输入的值，通过非线性转换映射到另一个值，这也是线性模型具有强大学习能力的关键所在。

## 常用激活函数
### Sigmoid
sigmoid是最常见的一个激活函数，但是是我最不常用的一个，为什么呢？因为sigmoid在激活函数家族中就像一个熊孩子，**缺点很多！** 听我慢慢道来...
先来看一下sigmoid长得如何：
![](https://img-blog.csdnimg.cn/20200710155150668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTY1ODEzMQ==,size_16,color_FFFFFF,t_70#pic_center)

>性质：
>1 sigmoid将所有的输入z都压缩到了区间 [0, 1] 之间;
>2 越靠近两端，函数的变化越平缓，梯度值越接近0；
>3 输入z的值大约在[-1, 1]之间时，sigmoid的梯度才处于正常状态，此时在z=0时梯度值最大，为0.25 (可自行求导验证，很简单)；

了解了sigmoid之后，来细数一下它的**缺点**：

> 缺点：
> 1 输出恒大于0，会改变输入z的分布，使其均值趋向于0.5，不利于下一层学习；
> 2 梯度正常的区间太小，局限在[-1, 1]。对于大部分的输入z，其值都不会处于该区间，导致对应的梯度值较小。在误差反向传播时，链式求导会出现多个<1的值连乘，梯度越来越小，导致梯度消失问题；
>3 最大的梯度值为0.25，也就是说对于所有的输入z，其对应的梯度都<1，梯度消失问题无法避免。

其实sigmoid也不是一无是处，也是有一个**优点** 的：处处可导。尽管大部分激活函数都具有这个性质 (ReLU除外)。
### Tanh
介绍完了家族中的bad boy，再来看一下它的好兄弟Tanh。Tanh比sigmoid年长，自然会更成熟一点。(仅仅是**一点**)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710163916334.png#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710164225953.png#pic_center)

>性质：
>1 将输入x压缩在了 [-1, 1] 之间；
>2 其他的跟它的好兄弟sigmoid一毛一样。

来看一下它的**优缺点**：

> 优点：
> 1 解决了sigmoid输出均为正的缺点，会使输入x保持均值0的分布，性质要好于sigmoid，一般可以完美替代sigmoid。

>缺点：
>1 同sigmoid，都解决不了梯度消失问题。

### ReLU系列
对于梯度消失这个顽疾，它们的老大哥们成功解决了这个问题。先来看一下大哥们的尊荣：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710165207476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTY1ODEzMQ==,size_16,color_FFFFFF,t_70#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710165407664.png#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710165433832.png#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710165546713.png#pic_center)三者共同点：都有一个转折点（0 ，0），x>0 的部分都是 y=x 线性变换。三者唯一不同的部分就是在 x<0 的部分。
先来看一下正半轴这样设计的**优点**：

> 优点：
> 1 对于大于0的输入x, 对应位置的梯度都是1，解决了梯度消失问题；
> 2 梯度值比Tanh与sigmoid大，所以梯度下降更新权重时，收敛的更快。

接下来依次介绍一下三者负半轴的设计理念：

> 1 ReLU解决了梯度消失问题，但是其负半轴输出恒为0，对于小于0的输入z，神经元不会有输出。误差反向传播时也不会对该神经元的权重进行更新，如果某个神经元的输出多次为0，则会导致其权重一直得不到更新，类似神经元失活。还有一个问题就是ReLU在原点处不可导；
> 2 LeakReLU发现了负半轴恒为0的问题，所以它选择保留一部分输出。负半轴有了一个很小的梯度，一般为0.001或者更小，保留一部分输出使得权重多少都会有一点更新。但是它仍然没有意识到原点处不可导的问题；
> ELU同时意识到了存在的两个问题，它把负半轴替换成了一个非线性曲线，该曲线保留了一定的梯度，并且在原点处与y=x完美衔接，(0, 0)处有了属于自己的导数，此时存在的两个缺陷就都解决了。

其实还有一个亲戚 PReLU，它是在LeakyReLU上做的改进。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200710172504524.png#pic_center)LeakyReLU在负半轴乘上了一个很小的系数$\lambda$，这是一个定值。PReLU把这个定值改成了可以让模型自己学习得到的值$\alpha$, 使得负半轴的梯度跟随数据自适应的进行调整，显然更合理一些。

## 总结
上面介绍了那么多激活函数，大概可以分为两类：

> 1 Sigmoid、 Tanh
> 2 ReLU、LeakyReLU、ELU、PReLU 
> [注]： 第一类容易造成梯度消失问题，第二类的性质要优于第一类

在实际的使用中，我得出了几点结论：

> 1 一般只在输出层使用sigmoid，比如在处理二分类任务时，用sigmoid压缩得到的[0, 1]之间的值当做概率进行输出。网络内部层的激活函数使用其他性质更好的函数；
> 2 能用Tanh尽量不用Sigmoid, 能用ReLU尽量不用Tanh；
> 3 至于LeakyReLU、ELU、PReLU，在实际的使用中效果都跟ReLU差不多，可以通过尝试进行选择。

这篇文章是我对知识的一个回顾，也是我的处女作。如有问题，欢迎指出，评论or私信都可，我会及时回复滴。
也可参考我的知乎同名文章：[知乎](https://www.zhihu.com/people/yu-yi-chu-shi/posts)












